#RNN循环神经网络相关知识
#之前接触的数据 shape(1,n) X[x0,x1,...,xn]
#带有连续结构的可按时间步的数据 shape(t,n)  其中t是时间步个数 
#其中连续结构是说 把数据按照时间步拆分后 上下时间步仍然具有关联性
# 因此 有些图像也可以用RNN处理
    #比如 28*28的用01矩阵表示的手写体略缩图 把高拆分成28个时间步
    #因为如果把这28个时间步的数据不按照特定顺序排列 就不会的到对应的手写体
    #所以用RNN做这样的图像是成立的
# X[X0[x0,x1,..,xn],X2[x0,x1,..,xn],....,Xt[x0,x1,..,xn]]


#输入input 上一时间步的输出output_t-1(nn memory) 和 当前时间步的输入input_Xt
#输出output input经过NN运算的结果
# 2个相同的输出 1个作为下一时间步的输入(nn memory) 1个作为当前时间步的输出
# 因此output H_t = act(W_ih@X_t + W_hh@H_t-1 + bias) h表示自身
#因此 RNN是自带一次tanh激活运算的
#全连接层和RNN的区别主要在于 RNN把每次经过NN运算的结果又传给自身
#本质是一种递归神经网络
#很难优化的缺点在于 RNN的在时间步上的当前时间步的开始要等上一时间步运算的结束


#RNN参数
#num_layers 
#多层RNN 单层RNN结构在并行方向上的堆叠
#nonlinearity 默认激活函数tanh 可切换
#dropout正则
#bidirectional 双向 利于上下文信息处理 把ht到h1再算一次 
# 其中 output的输出结果是正向反向结果的拼接

#梯度爆炸 梯度消失
#梯度爆炸 深层网络中模型梯度是由多个梯度连乘计算得到的 
    #   模型参数特征值大于1 导致梯度称指数级增长 出现梯度爆炸
    #   模型参数特征中小于1 导致梯度乘指数级衰减 出现梯度消失
#影响 梯度爆炸导致模型训练震荡 梯度消失导致模型前层网络无法有效更新 无法捕获长距离依赖
# 缓解方法 
# 采用合适的激活函数 梯度裁剪(L正则) 缓解梯度爆炸 
# 门控机制调整梯度流动 BN/LN归一化 缓解梯度消失

#梯度裁剪 torch.utils.clip_grad_norm_(model.parameters(),max_norm)
#其中指定模型梯度最大取值为max_norm

#门控机制 GRU门控循环神经网络 LSTM长短期记忆 
# 都是通过设置门增加模型参数和模型复杂度 来防止梯度消失解决无法捕获长距离依赖的问题
#  都是把线性运算结果经过sigmoid做门控机制再将其参与到模型运算中实现对内容的去留


#RNN循环神经网络相关知识
#之前接触的数据 shape(1,n) X[x0,x1,...,xn]
#带有连续结构的可按时间步的数据 shape(t,n)  其中t是时间步个数 
#其中连续结构是说 把数据按照时间步拆分后 上下时间步仍然具有关联性
# 因此 有些图像也可以用RNN处理
    #比如 28*28的用01矩阵表示的手写体略缩图 把高拆分成28个时间步
    #因为如果把这28个时间步的数据不按照特定顺序排列 就不会的到对应的手写体
    #所以用RNN做这样的图像是成立的
# X[X0[x0,x1,..,xn],X2[x0,x1,..,xn],....,Xt[x0,x1,..,xn]]


#输入input 上一时间步的输出output_t-1(nn memory) 和 当前时间步的输入input_Xt
#输出output input经过NN运算的结果
# 2个相同的输出 1个作为下一时间步的输入(nn memory) 1个作为当前时间步的输出
# 因此output H_t = act(W_ih@X_t + W_hh@H_t-1 + bias) h表示自身
#全连接层和RNN的区别主要在于 RNN把每次经过NN运算的结果又传给自身
#本质是一种递归神经网络
#很难优化的缺点在于 RNN的在时间步上的当前时间步的开始要等上一时间步运算的结束

#RNN文本分类
#基本流程：
    # ->文本分词(文本预处理)
    # ->构建词典索引(文本结构化转换)
    # ->词向量embedding(分类器设计)
#语料->分词->vocab->vocab_idx->embedding(初始化ebd矩阵->wordSeq_idxSeq->ebd)->rnn->linear->
#SentencePiece 无监督的文本分词器和去分词器 核心是BPE算法 需要预先设定词汇量token数量
#其中 token可以理解为一个词向量对应的内容
#返回2个属于输入文档这一特定语境的分词模型和词典表
#1个input文档的词典表 和 1个对应当前input文档的模型
#BPE算法 自底向上的字词分词算法 
#从最小字符单元出发 反复合并语料中出现频率最高的字符对 形成子词 
# 直到达到预设词汇表大小
#解决未登录词oov 和 词汇表大小权衡 问题


# pytorch 模型相关训练要素

# 全连接神经网络模型

# 1激活函数
# 1.1 sigmoid 早期提出 常用于逻辑回归及层次不深的NN中 值域(0,1)
# 1.2 tanh 早期提出 sigmoid的变体 值域(-1,1) 比sgmd更易训练 常用于RNN等深度NN
# 1.3 ReLu 12年提出 广泛用于CNN RNN等 是深度神经网络的主流激活函数
     # 计算高效 缓解sigmoid梯度消失线性 出现神经元死亡问题 负区梯度0 权重无法更新
# 1.4 leaky ReLU 解决ReLU神经元死亡问题 负区保留小梯度 保持活性
# 1.5 swish 含参激活函数 适用于大模型高精度深度神经网络
#以上激活函数处理sigmoid和tanh 其他值域都在(-1,+∞)

# 2归一化
# 2.1 实质 是一种带有参数的计算公式 把原数据规范化标准化成符合特定分布的数据
# 2.2 目的作用 稳定训练过程 加速模型收敛 减少过拟合 缓解梯度消失爆炸
# 2.3 原理 对于发生内部协变量偏移的数据进行归一化操作减少内部协变量偏移
# 协变量偏移(由于参数更新导致输入数据分布发生变化)
# 2.4 公式 标准化+缩放平移 
# 2.5 归一化操作是含参的 参数可训练 归一化操作不改变原有数据样本特征
# 2.5 因为含参 所以要在不同层与层之间分别定制 不可像激活函数一样复用

#3正则化 以Dropout为代表
# 3.1 实质 一种数据筛选手段
# 3.2 目的 抑制过拟合 提高泛化能力 近似模型集成效果
# 3.4 原理 随机失活 + 缩放补偿
    #随机失活 通过在训练过程中随机置零一些神经元 
        # 使得模型每次选择随机的一些神经元参与训练 使得NN无法过度依赖某些特定特征
        #以指定概率p损失当前层的一些神经元(让其输出置零) 
    #缩放补偿 测试(测试阶段不调用dropout)输出乘1/(1-p) 或训练输出乘p
# 3.5 不含可训练参数 具有复用性 

#4 model.train() 让归一化层正则化层生效 model.eval()让归一化层正则化层失效
# 模型中包含正则化归一化层写.train() .eval()才生效

#5优化器optimizer
# 5.1 实质 神经网络更新模型所有参数的工具 
# 需要获取模型参数和学习率
# 5.2 功能 获取模型所有参数梯度 更新模型参数 清空模型所有参数梯度
#SGD 每次使用单个样本计算梯度 收敛路径曲折震荡 
    # 单轮迭代速度快 总训练速度慢 易陷入局部最优解
#BGD 使用所有样本计算梯度 不适合大样本大模型 易跳出局部最优得到全局最优
#mini-BGD SGD和BGD的折中方案 具有较稳定较快的特点
# 在pytorch中所说的SGD是 是SGD+MBGD
# 权重衰减namda 在参数更新时惩罚参数大小 约束模型参数大小 约束vc维大小 提升泛化能力
# SGD+momentum 动量影响梯度下降的方向和大小 
    # 使得每次参数更新方向 不仅取决于当前方向还取决于上一次参数更新的方向
    # 这样累加 到t次参数更新 当前梯度大小方向是受前t-1次所有梯度大小方向影响的
    # 优势 梯度方向一致加速收敛 梯度方向不同抑制震荡
#RMSProp 带归一化梯度的参数更新 带动量效果
#自适应矩估计Adam 一阶矩二阶矩
    # 优势 自适应调整学习率 稳定训练过程 超参数鲁棒
#AdamW 采用独立权重衰减策略的Adam 与Adam区别在于将权重衰减和梯度计算分开进行

#6神经网络的输出与模型任务
#6.1 单节点输出 回归问题 输出一个数值
#6.2 多节点输出 分类问题 输出多个概率其和=1 取预测值与真实值误差最小的概率再对应到类别

#7损失函数Loss
# 7.1回归任务相关loss 
# MSE均方误差 平方放大误差影响 平均反应平均差异程度 即L2损失 易受误差大异常值影响
# MAE平均绝对误差 取误差绝对值再平均 即L1损失 对误差大的异常值鲁棒性比L2更强
    #由于绝对值 可导性不如L2损失 收敛速度较慢 
    #对误差小的值效果不是很好
# Huber损失 L1 L2损失的折中方案 预测值和真实值误差小时取L2 大时取L1 即smooth L1损失
    #这样操作 在模型损失接近极值时会切换到L1 效果更好
#7.2分类任务相关loss
# 交叉熵损失 croosentropyloss
# 信息量 概率越小信息量越大
# 信息熵 信息量的加权平均 期望
# 相对熵(KL散度) 同一随机变量x的真实分布与预测分布的差异 
    # KL散度越小 差异越小 预测越准
#交叉熵 KL散度公式拆开去掉真实值计算部分(因为此部分是固定值log1=1=0) 得到交叉熵 
#交叉熵 加和(-plog(q))其中p是真实分布比如[1,0,0] q是预测分布比如[0.7,0.2,0.1]
#由于[1,0,0] 因此交叉熵计算的只是真实值对应项的自信息量676
#crossentropyloss = ①softmax + ②NLLLoss(负对数似然损失)
    #因为交叉熵公式 加和(-plog(q)) p q都是概率分布各自加和为1 
    #要算交叉熵 先有概率分布q 所以先用softmax
    #而负对数似然 是交叉熵公式中q外面的东西
#binary_crossentropyloss 二分类交叉熵 = 伯努利分布似然函数Log取-负 即做的是逻辑回归任务


